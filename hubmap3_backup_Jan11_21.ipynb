EfficientNetB4=tf.keras.applications.EfficientNetB4




#encoder7 to decoder7:
N=24
input_shape=(224,224,3)
encoder = EfficientNetB4(weights='imagenet',
                            include_top=False,
                            input_shape=input_shape)



l0=[l.name for l in encoder.layers]
l1 = [s.split(" ")[0] for s in l0]
blocks_in_l1 = np.where(["block" in l for l in l1])[0]

l2 = [s.split("block")[1].split("_")[0][0] for s in [l1[b] for b in blocks_in_l1]]
    

p=0
ll=[]
for i in range(len(l2)):
    
    if l2[i]!=p:
        p=l2[i]
        print(i)
        ll.append(i)

ll.append(len(l2))  

layerand=(2,10)

log=dict()
for ii in range(150):
    log[ii]=dict()
    log[ii]["acc"]=[]
    log[ii]["dice2"]=[]
    log[ii]["dice"]=[]
    log[ii]["loss"]=[]
    
    input = encoder.input
    
    n7=np.random.randint(layerand[0],layerand[1])
    
    log[ii]["enc7"]=f"{n7},{encoder.layers[ll[-1]+n7].name},shape: {encoder.layers[ll[-1]+n7].output.shape}"
    enc7 = encoder.layers[ll[-1]+n7].output

    dec7 = Conv2DTranspose(N, (2,2),strides=(2,2))(enc7)
    dec7 = Conv2D(N, (2,2),strides=(2,2))(dec7)
    dec7 = Conv2DTranspose(N, (2,2),strides=(2,2))(dec7)
    
#     if np.random.randint(2) == 1:
#         dec7 = BatchNormalization()(dec7)
#         log[ii]["dec7 bn"]=True
#     else:
#         log[ii]["dec7 bn"]=False

    #concatenate encoder5:
    
    n5 = 6#np.random.randint(layerand[0],layerand[1])
    log[ii]["enc5"]=f"{n5},{encoder.layers[ll[-3]+n5].name},shape: {encoder.layers[ll[-3]+n5].output.shape}"
    
    enc5 = encoder.layers[ll[-3]+n5].output

    concat = concatenate([dec7,enc5])

    dec5 = Conv2DTranspose(N, (2,2),strides=(2,2))(concat)
    dec5 = Conv2D(N, (2,2),strides=(2,2))(dec5)
    dec5 = Dropout(0.2)(dec5)
    dec5 = Conv2DTranspose(N, (2,2),strides=(2,2))(dec5)
#     if np.random.randint(2) == 1:
#         dec5 = BatchNormalization()(dec5)
#         log[ii]["dec5 bn"]=True
#     else:
#         log[ii]["dec5 bn"]=False
    
    n3 = 6#np.random.randint(layerand[0],layerand[1])
    log[ii]["enc3"]=f"{n3},{encoder.layers[ll[-5]+n3].name},shape: {encoder.layers[ll[-5]+n3].output.shape}"
    
    enc3 = encoder.layers[ll[-5]+n3].output

    concat = concatenate([dec5,enc3])

    dec3 = Conv2DTranspose(N, (2,2),strides=(2,2))(concat)
    dec3 = Conv2D(N, (2,2),strides=(2,2))(dec3)
    #dec3.shape[3]
    #dec3 = RandomWeightedAverage(dec3.shape[3])(dec3)
    dec3 = Conv2DTranspose(N, (2,2),strides=(2,2))(dec3)

    
    
#     if np.random.randint(2) == 1:
#         dec3 = BatchNormalization()(dec3)
#         log[ii]["dec3 bn"]=True
#     else:
#         log[ii]["dec3 bn"]=False
    dec3.shape

    n2 = np.random.randint(layerand[0],layerand[1])
    log[ii]["enc2"]=f"{n2},{encoder.layers[ll[-6]+n2].name},shape: {encoder.layers[ll[-6]+n2].output.shape}"
    
    enc2 = encoder.layers[ll[-6]+n2].output
    concat = concatenate([dec3,enc2])

    dec2 = Conv2DTranspose(N, (2,2),strides=(2,2))(concat)
    dec2 = Conv2D(N, (2,2),strides=(2,2))(dec2)
    dec2 = Conv2DTranspose(N, (2,2),strides=(2,2))(concat)
    #dec2 = Conv2D(N, (2,2),strides=(2,2))(dec2)
    #dec2=BatchNormalization()(dec2)

#     if np.random.randint(2) == 1:
#         dec2 = BatchNormalization()(dec2)
#         log[ii]["dec2 bn"]=True
#     else:
#         log[ii]["dec2 bn"]=False
    # dec2act = layers.Activation("relu")(dec2)
    # enc117=encoder.layers[117].output
    # c117 = concatenate([enc117,dec2act])
    # #dec2 = MaxPooling2D((2,2))(dec2)
    # #print("b: ",dec2.shape)

    # dec2=Conv2D(N,(1,1),strides=(4,4))( dec2)
    # dec2.shape
    # #c117.shape
    # # concat117 = concatenate([dec2,c117])
    # # concat117 = Conv2D(enc117.shape[3],(1,1))(concat117)
    # # # enc117 = Conv2D(N,(1,1))(enc117)
    # # av117=tf.keras.layers.Average()([concat117, enc117])
    # # av117=Conv2DTranspose(N,(1,1),strides=(4,4))(av117)

    # # avactdec2 = tf.keras.layers.Average()([dec2act, av117])

    # # dec2 = Conv2D(N, (2,2),strides=(2,2))(dec2)
    # dec2=Conv2DTranspose(N,(1,1),strides=(4,4))(dec2)
    # dec2 = Conv2DTranspose(N, (2,2),strides=(8,8))(dec2)
    n0 = np.random.randint(layerand[0],layerand[1])
    log[ii]["enc0"]=f"{n0},{encoder.layers[ll[1]+n0].name},shape: {encoder.layers[ll[1]+n0].output.shape}"
    
    enc0 = encoder.layers[ll[1]+n0].output


    concat = concatenate([dec2,enc0])

    #avactdec2=Conv2DTranspose(concat.shape[3],(1,1),padding='same')(avactdec2)
    #concat=tf.keras.layers.Add()([avactdec2,concat])



    #print("b: ",concat.shape)
    #concat = layers.Activation("relu")(concat)
    #print("a: ",concat.shape)

    dec0 = Conv2DTranspose(N, (2,2),strides=(2,2))(concat)
    dec0 = Conv2D(N, (2,2),strides=(2,2))(dec0)
    dec0 = Conv2DTranspose(N, (2,2),strides=(2,2))(dec0)
    
    
    
    if np.random.randint(2) == 1:
        dec0 = BatchNormalization()(dec0)
        log[ii]["dec0 bn"]=True
    else:
        log[ii]["dec0 bn"]=False
    #dec0 = BatchNormalization()(dec0)
    output = Conv2D(1, 3, padding="same", activation="sigmoid")(dec0)



    model = keras.Model(input, output)
    #model.summary()




    #%%time
    from matplotlib.pyplot import imshow
    import matplotlib.pyplot as plt
    from copy import deepcopy

    # Instantiate an optimizer + scheduler.
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-4,
        decay_steps=7,
        decay_rate=0.9)
    #optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)
    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)#1e-3)
    # Instantiate a loss function.
    loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)


    #loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    #binary_crossentropy(y_batch_train,logits)#from_logits=True)

    train_acc_metric = keras.metrics.BinaryAccuracy()

    batch_size=14
    epochs=50
    N=500#700
    stepstop=N//batch_size*4
    ##############
    print("gathering data...")
    ls=getfiles0()
    lsr=getfiles1(ls)
    inds=[]
    dictinds=dict()
    print("gathering data... done")

    #inds_prev=set()


    print("log: ",log[ii])
    for e in range(epochs):
      if e>0:
            if np.mean(loss)>1.0 or np.mean(acc)<0.77:
                log[ii]['break']=True
                print('BREAK (epoch)')
                break;
            

      print("EPOCH # = ",e)

    #   (tiles,masks)=generate_data_for_epoch(n=1000,ranim=True)

    #   print("ds length = ", len(tiles))



    #   tl2=snaps(margin=[5000,5000],n=350)

    #   tiles=[]

    #   while len(tl2)==0:
    #       tl2=snaps(margin=[5000,5000],n=350)
    #   print("# of SAMPLES: ",len(tl2))
    #   (tiles,ys,perInput, masks) = getDataForBatch(tl2)


    #   train_tiles=list(train_tiles)[0]
    #   train_masks=list(train_masks)[0]


    #   train_tiles=list(train_tiles)
    #   train_masks=list(train_masks)

      for enum in range(2):
          if enum>0 or e>0:
              if np.mean(loss)>1.0 or np.mean(acc)<0.77:
                    log[ii]['break']=True
                    print('BREAK (enum)')
                    break;
          step=0
          #inds_prev=deepcopy(inds)
          inds_prev=deepcopy(inds)
          (tiles,masks,inds)=getfiles2(lsr,N)

          print("percentage of new data vs previous (sub) epoch: ",len(list(set(inds).difference(inds_prev)))/len(list(set(inds))))
          print("common indices: ",set(inds).difference(set(inds).difference(inds_prev)))
          gc.collect()


          print("sub epoch = ", enum)
          (train_tiles,test_tiles,train_masks,test_masks)=gen_train_test(tiles,masks)
          print("train ds length = ", len(train_tiles))
    #       batchtiles=np.vstack(tuple(train_tiles)).reshape(-1,224,224,3)

    #       masks3d=np.reshape(train_masks, np.array(train_masks).shape + (1,))
    #       batchmasks=np.vstack(tuple(masks3d)).reshape(-1,224,224,1)
    #         #batchx=np.vstack(tuple(tiles)).reshape(-1,150528)
    #         #np.vstack(tuple(ys)).reshape(8,224,224,3)
    #         #y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))


    #     #   ysx=tf.keras.utils.to_categorical(ys, num_classes=2)
    #     #   batchy=np.vstack(tuple(ysx))#.reshape(10,)



    #       #per=np.vstack(tuple(perInput))

    #         #batchtiles.shape
    #       trainbatch = tf.data.Dataset.from_tensor_slices((batchtiles,batchmasks))
    #       trainbatch = trainbatch.shuffle(buffer_size=10).batch(10)


    #         ##############

    #         # add simply as inputs: (tiles, masks)

          dice_acc=[]; dice_acc2=[]; dice_acc3=[]; acc=[]; loss=[]
          #############################AUGMENTATION################################################
          #https://keras.io/api/preprocessing/image/
          #Example of transforming images and masks together.
          ImageDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator
          # we create two instances with the same arguments
          data_gen_args = dict(featurewise_center=False,
    #                            featurewise_std_normalization=False,
                                rotation_range=90,
                               shear_range=0.2,
                              width_shift_range=0.1,
                              height_shift_range=0.1,
                                zoom_range=0.2,
                                horizontal_flip=True,
                                vertical_flip=True
                              )


          image_datagen = ImageDataGenerator(**data_gen_args)
          mask_datagen = ImageDataGenerator(**data_gen_args)

    #       image_datagen.fit(train_tiles)
    #       mask_datagen.fit(train_tiles)

          seed = np.random.randint(1000000)
          image_generator = image_datagen.flow(#_from_directory(
               train_tiles,seed=seed,batch_size=batch_size, shuffle=True)
        #     class_mode=None,
        #     
          mask_generator = mask_datagen.flow(#_from_directory(
               train_masks.reshape((-1,224,224,1)),seed=seed,batch_size=batch_size, shuffle=True)
             #train_masks,seed=se ed)
        #     class_mode=None,
          train_generator = zip(image_generator, mask_generator)
    #     #####################################################################################

          #while step<stestop:
          #for step, (x_batch_train, y_batch_train) in enumerate(trainbatch):
          for (x_batch_train, y_batch_train) in train_generator:
            step+=1
            if step>stepstop:
                break;



            #interpolated_img = RandomWeightedAverage(24)([x_batch_train[0:32].astype("float")])#, X_test[32:64].astype("float")])
            with tf.GradientTape() as tape:

    #             with tf.GradientTape() as tape:
    #     tape.watch(interpolated_img)
    #     y_pred = critic(interpolated_img)

              #  pass

    #           if step>stepstop:
    #             break;

              logits = model(x_batch_train, training=True)#[:,:,:,1:]
              #print(logits)


              loss_value = loss_fn(y_batch_train, logits)
              #print(loss_value)




            grads = tape.gradient(loss_value, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
            train_acc_metric.update_state(y_batch_train, logits)  
            # print("y_batch_train:         ",type(y_batch_train))
            # print("logits:    ", type(logits))
            dice_acc.append(tl.cost.dice_coe(tf.cast(logits,"double"),tf.cast(y_batch_train,"double")))
            #dice_acc2.append(get_dice_coeff(np.array(tf.cast(logits,"double")),np.array(tf.cast(y_batch_train,"double"))))
            dice_acc2.append(dice_coeff(np.array(tf.cast(logits,"double")),np.array(tf.cast(y_batch_train,"double"))))
            
            train_acc = train_acc_metric.result()
            acc.append(train_acc)  
            loss.append(loss_value)
            
            if step %5 ==0:
              imshow(x_batch_train[0,:,:,0])
              imshow(logits[0,:,:,0],cmap='jet', alpha=0.7)

              plt.show()
              imshow(x_batch_train[0,:,:,0])
              imshow(y_batch_train[0,:,:,0], cmap='jet', alpha=0.5)
              plt.show()

            if step % 2 == 0:
              gc.collect()
              print(f"step # {step} out of {stepstop}")
              print("Dice training acc for step: %.4f" % (float(dice_acc[-1]),))
              print("Dice2 training acc for step: %.4f" % (float(dice_acc2[-1]),))
              print("Training loss (for one batch) at step %d: %.4f"% (step, float(loss_value)))
              #train_acc = train_acc_metric.result()
              print("Training acc for step: %.4f" % (float(train_acc),))
              print("Seen so far: %s samples" % ((step + 1) * 64))
              #acc.append(train_acc)  
              #loss.append(loss_value)
              
          train_acc = train_acc_metric.result()
          print("Training acc over epoch: %.4f" % (float(train_acc),))
          print("Dice training acc over epoch: %.4f" % (float(np.mean(dice_acc)),))
          print("Dice2 training acc over epoch: %.4f" % (float(np.mean(dice_acc2)),))
          
          log[ii]["acc"].append(acc)
          log[ii]["dice2"].append(dice_acc2)
          log[ii]["dice"].append(dice_acc)
          log[ii]["loss"].append(loss)


          save_load_pickle("log7.pickle",log)
            # Reset training metrics at the end of each epoch
          train_acc_metric.reset_states()
          for i in inds:
                try:
                    dictinds[i]+=1
                except:
                    dictinds[i]=1

    #       if enum<2:

    #             inds_prev=deepcopy(inds)

    #             (tiles,masks,inds)=getfiles2(lsr,N)
    #             print("percentage of new data vs previous (sub) epoch: ",len(list(set(inds).difference(inds_prev)))/len(list(set(inds))))
    #             print("common indices: ",set(inds).difference(set(inds).difference(inds_prev)))
    #             #(tiles,masks)=generate_data_for_epoch(n=1000,ranim=False)

    #       #print("ds length = ", len(tiles))



# # #23:30